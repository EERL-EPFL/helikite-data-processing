{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data processing - Level 1\n",
    "Code written by Radiance and Yolanda (with the help of ChatGPT)"
   ],
   "id": "b2a582f6a53b6b34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib ipympl"
   ],
   "id": "851c7f4fdda59629",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "from helikite.constants import constants\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)"
   ],
   "id": "ea23e21f728e6af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:49:26.500384Z",
     "start_time": "2025-12-12T17:49:24.980692Z"
    }
   },
   "cell_type": "markdown",
   "source": "## Load data",
   "id": "a2a251ecd14cac9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helikite.metadata.utils import load_parquet\n",
    "\n",
    "df_level0, metadata = load_parquet(constants.LEVEL0_DIRPATH / f\"{constants.LEVEL0_FILE_BASENAME}.parquet\")"
   ],
   "id": "b5677b688b904c24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metadata",
   "id": "ced83562695d9b4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:49:27.558007Z",
     "start_time": "2025-12-12T17:49:27.545043Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Example commands to output different individual elements of the dataset**  \n",
    "\n",
    "*To use individual fields, just use the object (.) notation, for example*  \n",
    "  \n",
    "print(metadata.flight_date)  \n",
    "print(metadata.landing_time)  \n",
    "metadata.takeoff_time\n",
    "\n",
    "flight_computer_columns = [col for col in df.columns if col.startswith(\"flight_computer_\")]  \n",
    "print(flight_computer_columns)  \n",
    "\n",
    "smart_tether_columns = [col for col in df.columns if col.startswith(\"smart_tether_\")]  \n",
    "print(smart_tether_columns)"
   ],
   "id": "171e6ebf2d31bd27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DataProcessor class",
   "id": "689484c4e41f30e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helikite.classes.base import OutputSchemas\n",
    "from helikite.classes.data_processing_level1 import DataProcessorLevel1\n",
    "\n",
    "data_processor = DataProcessorLevel1(getattr(OutputSchemas, constants.OUTPUT_SCHEMA), df_level0, metadata)\n",
    "data_processor.state()\n",
    "\n",
    "# define your output directory here\n",
    "output_dir = constants.OUTPUTS_FOLDER / \"Processing\" / \"Level1\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "d8319c340c48d8fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Outlier removal\n",
    "\n",
    "To remove outliers, load the level 0 CSV file, making sure to set the index to the DateTime.  \n",
    "This function will load all the data, plot an individual variable, and then save a CSV of any outliers that are chosen as True. This outlier CSV will be later used by ```data_processor.set_outliers_to_nan()``` to mask the values in the original dataframe.\n",
    "\n",
    "_Note: No data is changed in the input dataframe._\n",
    "  \n",
    "In a first time, check the **'flight_computer_pressure'** against **'flight_computer_time'** as well as **'flight_computer_Out1_T'**, **'flight_computer_Out2_T'**, **'flight_computer_Out1_H'** and **'flight_computer_Out2_H'**.  \n",
    "Then check **'smart_tether_Wind (m/s)'**. The WD values corresponding to the WS outliers will automatically also be set as outliers – no need to manually select **'smart_tether_Wind (degrees)'** outliers (if ```use_coupled_columns``` is ```True```).\n",
    "If needed, remove **'flight_computer_Lat'** outliers. The Long values corresponding to the Lat outliers will automatically also be set as outliers – no need to manually select **'flight_computer_Long'** outliers (if ```use_coupled_columns``` is ```True```).\n",
    "\n",
    "### Coupled columns\n",
    "To check which columns are coupled, see output of ```data_processor.state()```. To add new groups of coupled columns to an instrument, pass the list of all the groups to argument ```coupled_columns```  of the\n",
    "instrument instance. For example:\n",
    "```\n",
    "flight_computer_v1 = FlightComputerV1(\n",
    "    name=\"flight_computer\",\n",
    "    ...\n",
    "    coupled_columns=[\n",
    "        ('flight_computer_TEMP1', 'flight_computer_RH1'),\n",
    "        ('flight_computer_TEMP2', 'flight_computer_RH2'),\n",
    "    ]\n",
    ")\n",
    "```\n",
    " For the coupled columns to be updated, restart the kernel and rerun the cells."
   ],
   "id": "ccffdf7a58849191"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "outliers_file = output_dir / f\"level1_{constants.FLIGHT_BASENAME}_outliers.csv\"\n",
    "data_processor.choose_outliers(y=\"flight_computer_pressure\", outliers_file=outliers_file, use_coupled_columns=True)\n",
    "# data_processor.choose_outliers(y=\"pops_pressure\", outliers_file=outliers_file, use_coupled_columns=True)\n",
    "# data_processor.choose_outliers(y=\"cpc_DateTime\", outliers_file=outliers_file, use_coupled_columns=True)"
   ],
   "id": "284c84003c1e1d1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:49:34.152120Z",
     "start_time": "2025-12-12T17:49:33.932507Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "If coupled columns were not specified when choosing outliers, ensure consistency manually.\n",
    "If coupled columns were specified, this adjustment is applied automatically."
   ],
   "id": "2980ae1d6219539a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# outliers = pd.read_csv(OUTLIER_FILEPATH, index_col=0, parse_dates=True)\n",
    "# outliers[\"smart_tether_Wind (degrees)\"] = outliers[\"smart_tether_Wind (m/s)\"]   # Remove WD values corresponding to outlying WS\n",
    "# outliers[\"flight_computer_Long\"] = outliers[\"flight_computer_Lat\"]              # Remove Long values corresponding to outlying Lat\n",
    "# outliers.to_csv(OUTLIER_FILEPATH, date_format=\"%Y-%m-%d %H:%M:%S\")              # Save corresponding outliers into the csv file"
   ],
   "id": "a68092abefb88143",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set GPS data in case of missing FC files\n",
    "data_processor.fillna_if_all_missing({\"flight_computer_Lat\": 7039.724, \"flight_computer_Long\": 817.1591})"
   ],
   "id": "e15c5caead50cfbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.state()",
   "id": "ab1f965b48702df7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_processor.set_outliers_to_nan()\n",
    "#df.loc[\"2025-02-15 09:47:40\":\"2025-02-15 09:47:50\", 'smart_tether_Wind (m/s)']   # Print time range to control if values replaced by NaN"
   ],
   "id": "bfc87422e1c9cbed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "# df['flight_computer_pressure'] = df['pops_pressure']"
   ],
   "id": "9f2baaa441f16229",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:49:54.847406Z",
     "start_time": "2025-12-12T17:49:54.575405Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Outlier removal double check\n",
    "Plot variables with possible removed outliers."
   ],
   "id": "c1453be577e47ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.plot_outliers_check()",
   "id": "b9f4404bb98219e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Check position of balloon compared to the station based on GPS coordinates.  \n",
    "\n",
    "Transformation of degrees-minutes coordinates (DM) into decimal degrees (DD) coordinates.  \n",
    "Addition of 'latitude_dd' and 'longitude_dd' into df."
   ],
   "id": "fa46c56342ea46e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_processor.convert_gps_coordinates(lat_col='flight_computer_Lat', lon_col='flight_computer_Long',\n",
    "                                       lat_dir='S', lon_dir='W')\n",
    "data_processor.plot_gps_on_map(center_coords=(-70.6587, -8.2850), zoom_start=13)"
   ],
   "id": "e06b20ec3f439314",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## T and RH averaging\n",
    "\n",
    "Averages flight computer temperature and humidity data from the two T/RH sensors.\n",
    "If > K NaNs for one of the sensors, takes only the other one into account, where K is a NaN threshold, which can be specified in `T_RH_averaging`.\n",
    "\n",
    "Plots T and RH as a function of pressure.\n",
    "Smart Tether is also plotted as an indication but not taken into account for the averaging.\n",
    "\n",
    "Adds 'Average_Temperature' and 'Average_RH' into df."
   ],
   "id": "36fe4bb1c69e926e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# if columns are not specified, takes the default flight computer columns\n",
    "data_processor.T_RH_averaging(columns_t=None, columns_rh=None, nan_threshold=400)\n",
    "data_processor.plot_T_RH(save_path=output_dir / f\"Level1_{constants.FLIGHT_BASENAME}_T_RH_averaging.png\")"
   ],
   "id": "3c57e733d876639",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**In case the standard T and RH averaging is not working, and the ST measurements need to be taken into account, update the cell above by:**\n",
    "\n",
    "Applying any required corrections to the smart tether T and RH measurements.\n",
    "```\n",
    "data_processor.df['smart_tether_T (deg C)_corr'] = data_processor.df['smart_tether_T (deg C)'].ffill().bfill() - 0.2\n",
    "data_processor.df['smart_tether_%RH_corr'] = data_processor.df['smart_tether_%RH'].ffill().bfill() - 6.1\n",
    "```\n",
    "\n",
    "Specifying the columns to average in the function call, including smart tether T and RH measurements:\n",
    "```\n",
    "data_processor.T_RH_averaging(\n",
    "    columns_t=['flight_computer_Out1_T', 'flight_computer_Out2_T', 'smart_tether_T (deg C)_corr'],\n",
    "    columns_rh=['flight_computer_Out1_H', 'flight_computer_Out2_H', 'smart_tether_%RH_corr'],\n",
    "    nan_threshold=400,\n",
    ")\n",
    "```"
   ],
   "id": "eda43e71c96a4188"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Altitude calculation \n",
    "\n",
    "Adds 'DateTime', 'Pressure_ground', 'Temperature_ground' and 'Altitude' into df.\n",
    "\n",
    "**When FC started at the balloon height and not on the sledge:**\n",
    "specify the height of the sledge in meters in the `offset_to_add` parameter"
   ],
   "id": "e6216d2c5eae9787"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_processor.altitude_calculation_barometric(offset_to_add=0)\n",
    "data_processor.df.head()"
   ],
   "id": "63518fbb7a15ec5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.plot_altitude()",
   "id": "be5f639e98a3e8ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Processing missing instruments\n",
    "**If no measurements are available for an instrument, add columns with NaNs into the dataset.**"
   ],
   "id": "d110ad8cdee2619d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.add_missing_columns()",
   "id": "17334d94fadf0d3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CO2 data processing",
   "id": "9a7f607375d51d51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.process_CO2_STP(min_threshold=200, max_threshold=500)",
   "id": "32742ba7dbf34cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## STAP data processing",
   "id": "735a1a13d5374a52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.STAP_STP_normalization()",
   "id": "c4ef367790dcd9f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## POPS data processing\n",
    "### POPS total concentration calculation\n",
    "\n",
    "Calculate the total concentration and dN/dlogDP for each bin\n",
    "\n",
    "From Pohorsky et al. (2024) it appeared that particles with diameters between 142 and 186 (bins 0 to 2) are wrongly detected by the POPS as total particle concentration increases. This phenomenon can be explained by electronic noise from the detector, where fringes on the edge of the Gaussian signal are perceived as smaller particles by the software. It was therefore decided to only consider data for particles larger than 186 nm as the error induced by the first three bins is too high.\n",
    "\n",
    "*dN_pops = pops_bX / popsflow_mean = dN*  \n",
    "*pops_total_conc = sum of dN_pops*  \n",
    "*pops_bX_dlogDp = dN/dlogDp*  \n",
    "\n",
    "Adds 'pops_total_conc' and 'pops_bX_dlogDp' into df."
   ],
   "id": "49522a8bfdfd9790"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.POPS_total_conc_dNdlogDp()",
   "id": "4e28875d9e8852f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Apply in case of POPS outlier to replace by NaN at a specific timestamp**",
   "id": "122cd9a6e93b5ca9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: check 2025-02-12 07:57:25 + update coupled columns ? or change choose_outliers\n",
    "if False:\n",
    "    pops_b_cols = [col for col in df.columns if col.startswith('pops_b')]\n",
    "    cols_to_nan = pops_b_cols + ['pops_total_conc']\n",
    "\n",
    "    timestamp = pd.Timestamp(\"2025-02-12 07:57:25\")\n",
    "    df.loc[timestamp, cols_to_nan] = np.nan"
   ],
   "id": "94c8e4ddc58f8274",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Apply in case of intervals with POPS measurements = 0**  ",
   "id": "2d8b3f7b5d66ff38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: use choose_outliers ?\n",
    "if False:\n",
    "    import matplotlib.dates as mdates\n",
    "    from matplotlib.widgets import Button\n",
    "    import warnings\n",
    "\n",
    "    # Suppress specific warning\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    # Suppress all warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    %matplotlib widget\n",
    "    plt.close('all')\n",
    "\n",
    "    # Plot setup\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    palette = [\"#F54B0F\", \"#415067\"]\n",
    "\n",
    "    ax.plot(data_processor.df.index, data_processor.df[\"pops_total_conc\"], color=palette[0], linewidth=2)\n",
    "    ax.grid(True, ls=\"--\", alpha=0.5)\n",
    "    ax.set_ylim(-5, 400)\n",
    "    ax.set_ylabel(\"POPS_total_conc (/cm3)\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.xaxis.set_major_locator(mdates.MinuteLocator(interval=10))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "\n",
    "    # Interaction logic\n",
    "    selected_points = []\n",
    "    stable_periods = []\n",
    "    span_artists = []\n",
    "\n",
    "    def onclick(event):\n",
    "        if event.inaxes != ax:\n",
    "            return\n",
    "\n",
    "        # Skip if zoom or pan mode is active\n",
    "        if plt.get_current_fig_manager().toolbar.mode != '':\n",
    "            return\n",
    "\n",
    "        click_time = mdates.num2date(event.xdata)\n",
    "        selected_points.append(click_time)\n",
    "\n",
    "        ax.plot(event.xdata, event.ydata, 'o', color=palette[1], markersize=8)\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        if len(selected_points) == 2:\n",
    "            start, end = sorted(selected_points)\n",
    "            stable_periods.append((start, end))\n",
    "            span = ax.axvspan(start, end, color=palette[1], alpha=0.2)\n",
    "            span_artists.append(span)\n",
    "            selected_points.clear()\n",
    "            fig.canvas.draw()\n",
    "    results_df = None\n",
    "\n",
    "    def finish_selection(event):\n",
    "        global results_df\n",
    "\n",
    "        if stable_periods:\n",
    "            results_df = pd.DataFrame(stable_periods, columns=['Start_Time', 'End_Time'])\n",
    "            results_df['Duration'] = results_df['End_Time'] - results_df['Start_Time']\n",
    "\n",
    "            # Optional formatting\n",
    "            results_df['Start_Time'] = results_df['Start_Time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            results_df['End_Time'] = results_df['End_Time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            print(\"\\nSelected Stable Periods:\")\n",
    "            print(results_df.to_string(index=False))\n",
    "\n",
    "            # Copy to clipboard\n",
    "            results_df.to_clipboard(index=False)\n",
    "            print(\"\\nResults copied to clipboard!\")\n",
    "        else:\n",
    "            print(\"No stable periods selected\")\n",
    "\n",
    "    # Add button\n",
    "    ax_button = plt.axes([0.82, 0.01, 0.15, 0.05])\n",
    "    btn = Button(ax_button, 'Finish Selection', color='lightgoldenrodyellow')\n",
    "    btn.on_clicked(finish_selection)\n",
    "\n",
    "    # Hook up click handler\n",
    "    fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "\n",
    "    plt.title(\"Click to select stable periods\")\n",
    "    plt.show()\n",
    "\n",
    "    print(results_df)"
   ],
   "id": "296fbf07add64b84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if False:\n",
    "    # Save or load pops mask csv\n",
    "    results_df.to_csv(DATA_LEVEL1_DIRPATH / f\"Level1_{metadata.flight_date}_Flight_{metadata.flight}_POPSmask.csv\", index=False)\n",
    "    # results_df = pd.read_csv(DATA_LEVEL1_DIRPATH / f\"Level1_{metadata.flight_date}_Flight_{metadata.flight}_POPSmask.csv\")\n",
    "\n",
    "    # Mask pops data in selected time interval\n",
    "    pops_cols = [f'pops_b{i}_dlogDp' for i in range(16)] + ['pops_total_conc']\n",
    "\n",
    "    for _, row in results_df.iterrows():\n",
    "        start = row['Start_Time']\n",
    "        end = row['End_Time']\n",
    "        mask = (df.index >= start) & (df.index <= end)\n",
    "        df.loc[mask, pops_cols] = np.nan\n",
    "\n",
    "    df.loc[\"2025-02-10 13:47:00\":\"2025-02-10 13:47:15\", 'pops_b3_dlogDp']"
   ],
   "id": "3b2b217d06d72f66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Normalization of POPS concentrations to standard temperature and pressure (STP)\n",
    "\n",
    "at 0°C (273.15 K) and 1 atm (1013.25 hPa).  \n",
    "\n",
    "$C_{\\text{STP}} = C_{\\text{measured}} \\times \\left( \\frac{P_{\\text{measured}}}{P_{\\text{STP}}} \\right) \\times \\left( \\frac{T_{\\text{STP}}}{T_{\\text{measured}}} \\right)$  \n",
    "\n",
    "\n",
    "       \n",
    "Adds 'pops_total_conc_stp' and 'pops_bX_dlogDp_stp' into df."
   ],
   "id": "bf2904a6147c11ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.POPS_STP_normalization()",
   "id": "fafc12d7c2d4217b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Plot POPS size distribution and total concentration  \n",
    "\n",
    "STP normalized bin concentrations and total concentration"
   ],
   "id": "7837fee7fc9eda14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helikite.instruments.pops import plot_pops_distribution\n",
    "\n",
    "plot_pops_distribution(data_processor.df, time_start=None, time_end=None)"
   ],
   "id": "7ff3c54ffaf52ee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## mSEMS data processing\n",
    "### mSEMS total concentration calculation\n",
    "\n",
    "*msems_inverted_Bin_ConcX = dN/dlogDp*  \n",
    "*msems_inverted_dN_Bin_ConcX = conc * dlogDp*  \n",
    "*msems_inverted_dN_totalconc = sum of msems_inverted_dN_Bin_ConcX*  \n",
    "\n",
    "Adds 'msems_inverted_dN_Bin_ConcX' and 'msems_inverted_dN_totalconc' into df."
   ],
   "id": "b9e1e3269d0bc7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.mSEMS_total_conc_dN()",
   "id": "a6f815b54a8aa70a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Normalization of mSEMS concentrations to standard temperature and pressure (STP)\n",
    "\n",
    "at 0°C (273.15 K) and 1 atm (1013.25 hPa).  \n",
    "\n",
    "$C_{\\text{STP}} = C_{\\text{measured}} \\times \\left( \\frac{P_{\\text{measured}}}{P_{\\text{STP}}} \\right) \\times \\left( \\frac{T_{\\text{STP}}}{T_{\\text{measured}}} \\right)$  \n",
    "\n",
    "Adds 'msems_inverted_Bin_ConcX_stp' and 'msems_inverted_dN_totalconc_stp' to df."
   ],
   "id": "15d221a70552aa96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.mSEMS_STP_normalization()",
   "id": "7a95a6c662d0c5bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Plot mSEMS size distribution and total concentration\n",
    "\n",
    "STP normalized bin concentrations and total concentration"
   ],
   "id": "bed1b0a0167c2e61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.plot_msems_distribution(time_start=None, time_end=None)",
   "id": "4582af1fcc415bd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Replace no data or flooded mSEMS measurements by NaN**",
   "id": "4ab282d9ce2aad41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: use choose_outliers ?\n",
    "import numpy as np\n",
    "\n",
    "if False:\n",
    "    # Define time range of \"bad\" mSEMS measurements\n",
    "    start_time = \"2025-01-27 17:55\"\n",
    "    end_time = \"2025-01-27 19:45\"\n",
    "\n",
    "    inverted_cols = [\n",
    "        col for col in df.columns\n",
    "        if col.startswith(\"msems_inverted_\") and col != \"msems_inverted_DateTime\"\n",
    "    ]\n",
    "\n",
    "    time_mask = (df.index >= pd.to_datetime(start_time)) & (df.index <= pd.to_datetime(end_time))\n",
    "\n",
    "    affected_rows = set()\n",
    "\n",
    "    # Apply only within the time range\n",
    "    for col in inverted_cols:\n",
    "        col_mask = df[col].map(lambda x: isinstance(x, (int, float)) and not np.isnan(x))\n",
    "        full_mask = time_mask & col_mask\n",
    "        affected_rows.update(df.index[full_mask])\n",
    "        df.loc[full_mask, col] = np.nan\n",
    "\n",
    "    # Count affected rows\n",
    "    print(f\"{len(affected_rows)} rows had numeric value replaced by NaN in 'msems_inverted_...' columns within the time range.\")"
   ],
   "id": "a770dea354cf9465",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## mCDA data processing\n",
    "### mCDA bin concentrations, total concentration and normalization per bin width\n",
    "\n",
    "For bins 1 to 256 :  \n",
    "*mcda_dataB X = raw counts*  \n",
    "*mcda_dataB X_dN = counts / (flow rate * sampling interval) = concentration*  \n",
    "*mcda_dN_totalconc = sum of mcda_dataBX_dN*  \n",
    "*mcda_dataB X_dN_dlogDp = dN/dlogDp = mcda_dataBX_dN / dlogDp*\n",
    "\n",
    "Adds 'mcda_dataB X_dN', 'mcda_dN_totalconc' and 'mcda_dataB X_dN_dlogDp' into df."
   ],
   "id": "1db309a53f4b0543"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.mcda_concentration_calculations()",
   "id": "ea68c77ee4de08b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Normalization of mCDA concentrations to standard temperature and pressure (STP)\n",
    "\n",
    "at 0°C (273.15 K) and 1 atm (1013.25 hPa).  \n",
    "\n",
    "$C_{\\text{STP}} = C_{\\text{measured}} \\times \\left( \\frac{P_{\\text{measured}}}{P_{\\text{STP}}} \\right) \\times \\left( \\frac{T_{\\text{STP}}}{T_{\\text{measured}}} \\right)$  \n",
    "\n",
    "Adds 'mcda_dataB X_dN_dlogDp_stp' and 'mcda_dN_totalconc_stp' to df."
   ],
   "id": "ee8ca33d9da448e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.mCDA_STP_normalization()",
   "id": "4773f5d86138bc51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Plot mCDA size distribution and total concentration\n",
    "\n",
    "STP normalized bin concentrations and total concentration"
   ],
   "id": "ee17f076cec0a021"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helikite.instruments.mcda_instrument import MCDA_MIDPOINT_DIAMETER_LIST\n",
    "\n",
    "data_processor.plot_mcda_distribution(MCDA_MIDPOINT_DIAMETER_LIST)"
   ],
   "id": "85f6ba6cf4dad069",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Vertical droplet size distribution**",
   "id": "58d4fe4183c58fad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helikite.instruments.mcda_instrument import MCDA_MIDPOINT_DIAMETER_LIST\n",
    "\n",
    "data_processor.plot_mcda_vertical_distribution(MCDA_MIDPOINT_DIAMETER_LIST)"
   ],
   "id": "277445d9164a10b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CPC3007 data processing",
   "id": "fae277d0941e1d44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Normalization of CPC3007 concentrations to standard temperature and pressure (STP)\n",
    "\n",
    "at 0°C (273.15 K) and 1 atm (1013.25 hPa).  \n",
    "\n",
    "$C_{\\text{STP}} = C_{\\text{measured}} \\times \\left( \\frac{P_{\\text{measured}}}{P_{\\text{STP}}} \\right) \\times \\left( \\frac{T_{\\text{STP}}}{T_{\\text{measured}}} \\right)$  \n",
    "\n",
    "\n",
    "       \n",
    "Adds 'CPC_total_N_stp' into df."
   ],
   "id": "ef49ee2271af231d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.CPC_STP_normalization()",
   "id": "1646df5497f8e8df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Filter data check",
   "id": "416a12cc44303407"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# NO FILT DATA FROM FC --> READ IN THE FILT FILE AND PASTE IT INTO DF WITH CORRECT NAMES !!!!!",
   "id": "ebd1ab81838a3a06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if False:\n",
    "    filt = pd.read_csv(DATA_FLIGHT_DIRPATH / \"250127A3.TXT\", skiprows=13, delimiter='\\t')  # or use sep=',' if it's CSV\n",
    "\n",
    "    date_str = filt['#YY/MM/DD'].str.strip()\n",
    "    time_str = filt['HR:MN:SC'].str.strip()\n",
    "    combined = date_str + ' ' + time_str\n",
    "    filt['datetime'] = pd.to_datetime(combined, format='%y/%m/%d %H:%M:%S')\n",
    "    filt = filt.set_index('datetime', drop=False)\n",
    "    filt.columns = 'flight_computer_F_' + filt.columns.astype(str)\n",
    "\n",
    "    filt"
   ],
   "id": "627364299f721b5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if False:\n",
    "    df = df.join(filt, how='left')\n",
    "    df"
   ],
   "id": "3103f1750708eb83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helikite.processing.post.level1 import filter_data\n",
    "\n",
    "if False:\n",
    "    filter_data(df)"
   ],
   "id": "aca352bc77ed3ffc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**In case of broken filters, replace filter positions by 1.**",
   "id": "e12590dda4fce0d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if False:\n",
    "    df['flight_computer_F_cur_pos'] = 1\n",
    "    df['flight_computer_F_pump_pw'] = 0"
   ],
   "id": "ada22e98cb21681a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if False:\n",
    "    df.loc[df['flight_computer_F_cur_pos'] == 0, 'flight_computer_F_cur_pos'] = 1\n",
    "    df.loc[df['flight_computer_F_cur_pos'] == 2, 'flight_computer_F_cur_pos'] = 1\n",
    "    df.loc[df['flight_computer_F_cur_pos'] == 4, 'flight_computer_F_cur_pos'] = 1"
   ],
   "id": "d803ec3fd6b29b52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if False:\n",
    "    broken_filter_start = pd.Timestamp(\"2025-02-11 15:44:14\")\n",
    "    broken_filter_end = pd.Timestamp(\"2025-02-11 16:11:39\")\n",
    "    df.loc[df['flight_computer_F_cur_pos'] == 0, 'flight_computer_F_cur_pos'] = 1\n",
    "\n",
    "    df.loc[(df.index >= broken_filter_start) & (df.index <= broken_filter_end), 'flight_computer_F_cur_pos'] = 2.0\n",
    "    df.loc[(df.index >= broken_filter_start) & (df.index <= broken_filter_end), 'flight_computer_F_pump_pw'] = 37.0\n",
    "    df['flight_computer_F_cur_pos']"
   ],
   "id": "bac036fdd3aed299",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data quicklooks",
   "id": "908b74a73a42b75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_path = output_dir / f'Level1_{constants.FLIGHT_BASENAME}_Flight_{constants.FLIGHT}.png'\n",
    "\n",
    "# change if you want to use custom x-axis limits and ticks\n",
    "use_custom_xlim = False\n",
    "\n",
    "if use_custom_xlim:\n",
    "    # Limits for x-axis (T, RH, mSEMS, CPC, POPS, mCDA, WS, WD)\n",
    "    custom_xlims = {\n",
    "        'ax1': (-6, 2),\n",
    "        'ax2': (60, 100),\n",
    "        'ax3': (0, 1200),\n",
    "        'ax4': (0, 1200),\n",
    "        'ax5': (0, 60),\n",
    "        'ax6': (0, 60),\n",
    "        'ax7': (0, 12)\n",
    "    }\n",
    "\n",
    "    custom_xticks = {\n",
    "        'ax1': np.arange(-6, 3, 2),\n",
    "        'ax2': np.arange(60, 101, 10),\n",
    "        'ax3': np.arange(0, 1201, 200),\n",
    "        'ax4': np.arange(0, 1201, 200),\n",
    "        'ax5': np.arange(0, 61, 10),\n",
    "        'ax6': np.arange(0, 61, 10),\n",
    "        'ax7': np.arange(0, 13, 3)\n",
    "    }\n",
    "    data_processor.plot_flight_profiles(constants.FLIGHT_BASENAME, save_path, xlims=custom_xlims, xticks=custom_xticks)\n",
    "else:\n",
    "    data_processor.plot_flight_profiles(constants.FLIGHT_BASENAME, save_path)"
   ],
   "id": "64d44be54b423a76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_path = output_dir / f'Level1_{constants.FLIGHT_BASENAME}_SizeDistr_Flight_{constants.FLIGHT}.png'\n",
    "data_processor.plot_size_distr(constants.FLIGHT_BASENAME, save_path)"
   ],
   "id": "a3dcdf73ad798359",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Level 1\n",
    "**Save file containing all the columns (processed)**"
   ],
   "id": "3751cb06303aee8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_processor.export_data(filepath=output_dir / f'level1_{constants.FLIGHT_BASENAME}.csv')",
   "id": "71e893230d784bb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Random code bits\n",
    "### Remove outliers from the Smart Tether WS and WD datapoints\n",
    "\n",
    "This is a **sliding-window median filter** used for **outlier detection and removal**.\n",
    "- Look at a window of neighboring values around each data point (10 neighboring values)\n",
    "- Compare the current point to the median of this window.\n",
    "- If the point is significantly different (>35% away from the median), it's treated as an outlier and **replaced with NaN**.\n",
    "\n",
    "Applied on WS, the corresponding WD datapoints are then also removed."
   ],
   "id": "f57bdf2e757b5789"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helikite.instruments.smart_tether import wind_outlier_removal\n",
    "%matplotlib ipympl\n",
    "\n",
    "df_filtered = wind_outlier_removal(df)"
   ],
   "id": "2aba9c97b09b54b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**IF THE FILTER APPLIES CORRECTLY** : save the filtered WS and WD data back into th original dataframe",
   "id": "36c45c5c859b38bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the filtered data back into th original dataframe\n",
    "df['smart_tether_Wind (m/s)'] = df_filtered['smart_tether_Wind (m/s)']\n",
    "df['smart_tether_Wind (degrees)'] = df_filtered['smart_tether_Wind (degrees)']\n",
    "print(\"Filtered data saved to the original dataframe.\")"
   ],
   "id": "d318a855fb26e367",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Metadata dictionary",
   "id": "2f782e51885b4d91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Otherwise, to make a dictionary from the metadata:\n",
    "metadata_dict = metadata.model_dump()\n",
    "\n",
    "# Then use it as a normal Python dictionary\n",
    "metadata_dict['flight_date']"
   ],
   "id": "47315be3a0a42477",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The dataframe is unpacked into the 'df' variable from that function above\n",
    "df"
   ],
   "id": "5bfb890c8e71b6cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GPS coordinate check",
   "id": "1d1971cd82358d58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# First y-axis for Longitude\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Longitude', color=color)\n",
    "ax1.plot(df.index, df['flight_computer_Long'], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.grid(True)\n",
    "\n",
    "# Second y-axis for Latitude\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:green'\n",
    "ax2.set_ylabel('Latitude', color=color)\n",
    "ax2.plot(df.index, df['flight_computer_Lat'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title('Longitude and Latitude over Time')\n",
    "plt.show()\n"
   ],
   "id": "3dfa8b539d47d52c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# def convert_dm_to_dd(dm_value, direction):\n",
    "#     if pd.isna(dm_value):\n",
    "#         return None\n",
    "#     degrees = int(dm_value / 100)\n",
    "#     minutes = dm_value - degrees * 100\n",
    "#     dd = degrees + minutes / 60\n",
    "#     if direction in ['S', 'W']:\n",
    "#         dd *= -1\n",
    "#     return dd\n",
    "#\n",
    "# # Apply conversion to the entire column\n",
    "# df['latitude_dd'] = df['flight_computer_Lat'].apply(lambda x: convert_dm_to_dd(x, 'S'))\n",
    "# df['longitude_dd'] = df['flight_computer_Long'].apply(lambda x: convert_dm_to_dd(x, 'W'))\n",
    "# df['latitude_dd']"
   ],
   "id": "1d737f883a2388ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Export TAPIR data for Delphine ",
   "id": "5f47cb29be2840c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['mcda_dN_totalconc_stp'] = np.nan",
   "id": "bf624df4d50cfdc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a new DataFrame with the same DateTime index as df\n",
    "df_tapir = df.loc[:, ['Altitude', 'Average_Temperature', 'Temperature_ground', 'mcda_dN_totalconc_stp'] + \n",
    "                  [col for col in df.columns if col.startswith('tapir_')]]\n",
    "df_tapir['Altitude'] = df_tapir['Altitude'].round(2)\n",
    "df_tapir['Average_Temperature'] = df_tapir['Average_Temperature'].round(2)\n",
    "df_tapir['mcda_dN_totalconc_stp'] = df_tapir['mcda_dN_totalconc_stp'].round(2)\n",
    "\n",
    "df_tapir"
   ],
   "id": "5975b8371ac08bb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Example metadata as a dictionary (you can adjust this to your actual metadata object)\n",
    "metadata_lines = {\n",
    "    'Flight date' : metadata.flight_date,\n",
    "    'Flight number' : metadata.flight,\n",
    "    'Takeoff time' : metadata.takeoff_time,\n",
    "    'Landing time' : metadata.landing_time,\n",
    "    'Average_Temperature (in °C)' : 'average T from two temperature sensors',\n",
    "    'Temperature_ground (in K)' : 'extrapolated ground temperature based on T at takeoff and landing',\n",
    "    'mcda_dN_totalconc_stp (cm-3)' : 'droplet total concentration',\n",
    "    'Note' : 'there are two \"peaks\" in the temperature profile, I am however not yet sure if they are significant or outliers'\n",
    "}\n",
    "\n",
    "# Construct the dynamic filename\n",
    "filename = f\"{metadata.flight_date}_Flight{metadata.flight}_TAPIR.txt\"\n",
    "\n",
    "# Define your output directory (use raw string if needed)\n",
    "output_dir = 'tapir'\n",
    "\n",
    "# Combine the path and filename\n",
    "output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "# Save the file\n",
    "with open(output_path, 'w', newline='') as f:\n",
    "    for key, value in metadata_lines.items():\n",
    "        f.write(f\"# {key}: {value}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    df_tapir.to_csv(f, index=True)"
   ],
   "id": "77a3fcb435474ac6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "39a8e87269be308b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
